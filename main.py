import os, io, time, logging, json
from typing import List, Tuple
import requests
from PIL import Image, ImageOps, ImageFilter
import numpy as np
import cv2
from openai import OpenAI

from telegram import Update, ReplyKeyboardMarkup, KeyboardButton
from telegram.ext import ApplicationBuilder, CommandHandler, MessageHandler, ContextTypes, filters

# ====== Env ======
BOT_TOKEN       = os.environ["BOT_TOKEN"]
WEBHOOK_URL     = os.environ.get("WEBHOOK_URL")
OCR_API_KEY     = os.environ.get("OCR_API_KEY", "helloworld")
OPENAI_API_KEY  = os.environ["OPENAI_API_KEY"]

logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(name)s: %(message)s")
log = logging.getLogger("baccarat-bot")
client = OpenAI(api_key=OPENAI_API_KEY)

# ====== Reply Keyboard ======
REPLY_KB = ReplyKeyboardMarkup(
    [
        [KeyboardButton("é–‹å§‹é æ¸¬")],
        [KeyboardButton("èŠ"), KeyboardButton("é–’")],
        [KeyboardButton("ç¹¼çºŒåˆ†æ"), KeyboardButton("åœæ­¢åˆ†æ")],
    ], resize_keyboard=True, one_time_keyboard=False
)

# ====== OCR helpers ======
def preprocess_for_ocr(img_bytes: bytes) -> bytes:
    im = Image.open(io.BytesIO(img_bytes)).convert("RGB")
    w, h = im.size
    scale = 1.7 if max(w, h) < 1800 else 1.2
    im = im.resize((int(w * scale), int(h * scale)))
    im = ImageOps.autocontrast(im, cutoff=1).filter(ImageFilter.SHARPEN)
    buf = io.BytesIO(); im.save(buf, format="JPEG", quality=90, optimize=True)
    return buf.getvalue()

def ocr_space_image(img_jpg_bytes: bytes) -> str:
    url = "https://api.ocr.space/parse/image"
    files = {"file": ("image.jpg", img_jpg_bytes, "image/jpeg")}
    data = {
        "apikey": OCR_API_KEY, "language": "cht", "isOverlayRequired": False,
        "filetype": "JPG", "OCREngine": 2, "scale": True, "isTable": True,
    }
    try:
        r = requests.post(url, files=files, data=data, timeout=60)
        r.raise_for_status()
        js = r.json()
        if js.get("IsErroredOnProcessing"): return ""
        pr = js.get("ParsedResults", []);  return pr[0].get("ParsedText","") if pr else ""
    except Exception as e:
        log.exception("OCR exception: %s", e); return ""

def build_history_from_text(txt: str) -> List[str]:
    def normalize(s: str) -> str:
        out = []
        for ch in s:
            code = ord(ch)
            if 0xFF01 <= code <= 0xFF5E: ch = chr(code - 0xFEE0)
            elif code == 0x3000: ch = " "
            out.append(ch)
        return "".join(out)
    s = normalize(txt).upper().replace(" ", "")
    s = s.replace("è“","è—").replace("é‘","é’").replace("é–‘","é–’")
    hist: List[str] = []
    for ch in s:
        if ch in ("èŠ","åº„","R","B","Z"): hist.append("èŠ")
        elif ch in ("é–’","é—²","è—","è“","é’","P","X"): hist.append("é–’")
    return hist

# ====== Color-based beads detection ======
# Tunables (base on your 5 images)
ROI_BOTTOM_PCT = 0.45          # take bottom 45% of image as bead area
MIN_AREA, MAX_AREA = 20, 1800  # contour area filter
BIN_WIDTH = 22                 # px per column bin (approx grid cell width)

def crop_bead_region(img: np.ndarray) -> np.ndarray:
    h = img.shape[0]
    y0 = int(h * (1 - ROI_BOTTOM_PCT))
    return img[y0:, :]  # bottom strip

def mask_colors(hsv: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    # red (two ranges)
    red1 = cv2.inRange(hsv, (0, 90, 70), (10, 255, 255))
    red2 = cv2.inRange(hsv, (170, 90, 70), (180, 255, 255))
    red = cv2.bitwise_or(red1, red2)
    # blue
    blue = cv2.inRange(hsv, (100, 90, 70), (135, 255, 255))
    return red, blue

def find_centers(mask: np.ndarray, label: str) -> List[Tuple[int,int,str]]:
    kernel = np.ones((3,3), np.uint8)
    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel, iterations=1)
    cnts, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    pts = []
    for c in cnts:
        area = cv2.contourArea(c)
        if MIN_AREA <= area <= MAX_AREA:
            M = cv2.moments(c)
            if M["m00"] != 0:
                cx = int(M["m10"]/M["m00"]); cy = int(M["m01"]/M["m00"])
                pts.append((cx, cy, label))
    return pts

def column_index(x: int, x_min: int) -> int:
    return int(round((x - x_min) / max(BIN_WIDTH, 1)))

def beads_history_from_image(img_bytes: bytes) -> List[str]:
    arr = np.frombuffer(img_bytes, np.uint8)
    im = cv2.imdecode(arr, cv2.IMREAD_COLOR)
    if im is None: return []

    roi = crop_bead_region(im)
    hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)
    red, blue = mask_colors(hsv)
    pts = find_centers(red, "èŠ") + find_centers(blue, "é–’")
    if not pts: return []

    # normalize to ROI coords and sort into columns
    xs = [p[0] for p in pts]
    x_min = min(xs)
    cols = {}
    for x, y, lab in pts:
        col = column_index(x, x_min)
        cols.setdefault(col, []).append((x, y, lab))

    history: List[str] = []
    for col in sorted(cols.keys()):
        col_pts = sorted(cols[col], key=lambda p: p[1])  # top->bottom
        for _, _, lab in col_pts:
            history.append(lab)
    return history

# ====== GPT predict (short reason, gap>=2%) ======
def gpt_predict(history: List[str]) -> dict:
    n = len(history); last6 = history[-6:] if n >= 6 else history[:]
    prompt = (
        "ä½ æ˜¯ç™¾å®¶æ¨‚èµ°å‹¢åˆ†æå°ˆå®¶ï¼Œçµåˆã€å…¨å±€(60%)ã€‘èˆ‡ã€æœ€è¿‘6æ‰‹(40%)ã€‘é€²è¡Œæ··åˆåˆ†æï¼Œ"
        "é æ¸¬ä¸‹ä¸€å±€èŠ/é–’èˆ‡å‹ç‡ã€‚"
        "åš´æ ¼è¦æ±‚ï¼šå‹ç‡å·®è‡³å°‘0.02ï¼Œä¸å¾—50/50ã€‚åªå›JSONï¼š"
        "{pick:'èŠæˆ–é–’', p_bank:0~1, p_player:0~1, reason:'ç°¡çŸ­ä¸­æ–‡çµè«–', detail:'å®Œæ•´åˆ†æ'}ã€‚"
        "reason æœ€å¤š15å­—ã€‚p_bank+p_player=1ã€‚\n\n"
        f"å…¨å±€åºåˆ—ï¼ˆ{n}æ‰‹ï¼‰ï¼š{' '.join(history)}\n"
        f"æœ€è¿‘6æ‰‹ï¼š{' '.join(last6)}"
    )
    resp = client.chat.completions.create(
        model="gpt-4o-mini", temperature=0.3,
        messages=[{"role":"user","content":prompt}],
    )
    txt = resp.choices[0].message.content
    try:
        data = json.loads(txt)
    except Exception:
        log.warning("GPT éJSONï¼š%s", txt)
        data = {"pick":"èŠ","p_bank":0.51,"p_player":0.49,"reason":"è¶¨å‹¢å¹³è¡¡","detail":"fallback"}

    pb = float(data.get("p_bank", 0.51)); pp = float(data.get("p_player", 0.49))
    if abs(pb-pp) < 0.02:
        if pb >= pp: pb, pp = 0.51, 0.49
        else: pb, pp = 0.49, 0.51
    s = pb+pp
    if s != 1: pb = round(pb/s,2); pp = round(1-pb,2)
    return {
        "pick": "èŠ" if str(data.get("pick","èŠ")).startswith("èŠ") else "é–’",
        "p_bank": round(pb,2),
        "p_player": round(pp,2),
        "reason": (data.get("reason") or "ç¶œåˆæ¬Šé‡åˆ¤æ–·")[:15],
    }

def fmt(pred: dict) -> str:
    return (f"âœ… é æ¸¬ï¼š{pred['pick']}\n"
            f"ğŸ“Š å‹ç‡ï¼šèŠ {int(pred['p_bank']*100)}%ã€é–’ {int(pred['p_player']*100)}%\n"
            f"ğŸ§  çµ±åˆåˆ†æï¼š{pred['reason']}")

# ====== Handlers ======
async def cmd_start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text(
        "å‚³ç‰Œè·¯åœ–ç‰‡å»ºç«‹æ¨¡å‹ï¼›ä¹‹å¾Œç”¨ä¸‹æ–¹å¿«æ·éµæ“ä½œï¼š\n"
        "é–‹å§‹é æ¸¬ï¼èŠï¼é–’ï¼ç¹¼çºŒåˆ†æï¼åœæ­¢åˆ†æ",
        reply_markup=REPLY_KB
    )

async def on_photo(update: Update, context: ContextTypes.DEFAULT_TYPE):
    m = update.message
    await m.reply_text("ğŸ“¥ åœ–ç‰‡å·²æ¥æ”¶ï¼Œé–‹å§‹åˆ†æ...", reply_markup=REPLY_KB)

    file = await m.photo[-1].get_file()
    raw_bytes = await file.download_as_bytearray()

    # 1) å„ªå…ˆï¼šè‰²å¡Šåµæ¸¬
    hist = beads_history_from_image(bytes(raw_bytes))

    # 2) å¤±æ•—â†’é€€å› OCR
    if not hist or len(hist) < 6:
        jpg = preprocess_for_ocr(bytes(raw_bytes))
        txt = ocr_space_image(jpg)
        hist = build_history_from_text(txt)
        raw_preview = (txt[:200] + "â€¦") if len(txt) > 200 else txt
        await m.reply_text(f"ğŸ“ OCRåŸæ–‡ï¼ˆå‰200å­—ï¼‰ï¼š\n{raw_preview}")

    await m.reply_text(f"ğŸ“œ åµæ¸¬åˆ°æ­·å²ï¼š{' '.join(hist) if hist else 'ï¼ˆç©ºï¼‰'}")

    context.user_data["room"] = {
        "built_at": int(time.time()),
        "history": hist,
        "last_input": None,
    }
    await m.reply_text(
        "ğŸ§© æˆ¿é–“æ•¸æ“šåˆ†æå®Œæˆ âœ…\nğŸ§  GPT æ··åˆåˆ†ææ¨¡å‹å·²å»ºç«‹\n"
        "1ï¸âƒ£ æŒ‰ã€ŒèŠ/é–’ã€è¼¸å…¥æœ€æ–°é–‹ç\n2ï¸âƒ£ å†æŒ‰ã€Œç¹¼çºŒåˆ†æã€é æ¸¬ä¸‹ä¸€å±€\n"
        "ğŸ” æ›æˆ¿è«‹æŒ‰ã€Œåœæ­¢åˆ†æã€ã€‚",
        reply_markup=REPLY_KB,
    )

async def on_text(update: Update, context: ContextTypes.DEFAULT_TYPE):
    t = (update.message.text or "").strip()

    if t == "é–‹å§‹é æ¸¬":
        await update.message.reply_text(
            "âœ…å·²æ”¶åˆ°æ‚¨çš„è«‹æ±‚ï¼Œè«‹å‚³ç™¾å®¶èµ°å‹¢åœ–\n\n"
            "âš ï¸æ³¨æ„è«‹å‹™å¿…ä½¿ç”¨æ–‡è²¡æ¨è–¦å¹³å°ï¼Œå¦‚æœ‰åˆ¤æ–·å¤±èª¤é€ æˆå¤±åˆ©ï¼Œå¾Œæœè‡ªè² ï¼\n"
            "ä½¿ç”¨éæ¨è–¦å¹³å°ç™¼ç¾è€…æ°¸ä¹…åœç”¨æ©Ÿå™¨äººè³‡æ ¼ğŸš«",
            reply_markup=REPLY_KB,
        ); return

    if t == "åœæ­¢åˆ†æ":
        context.user_data.pop("room", None)
        await update.message.reply_text("ğŸ§¹ å·²æ¸…ç©ºè³‡æ–™ï¼Œè«‹é‡æ–°ä¸Šå‚³èµ°å‹¢åœ–ã€‚", reply_markup=REPLY_KB)
        return

    room = context.user_data.get("room")

    if t in ("èŠ","é–’"):
        if not room:
            await update.message.reply_text("å°šæœªå»ºç«‹æ¨¡å‹ï¼Œè«‹å…ˆä¸Šå‚³åœ–ç‰‡ã€‚", reply_markup=REPLY_KB); return
        room["last_input"] = t
        await update.message.reply_text("âœ… å·²è¨˜éŒ„æœ€æ–°é–‹çï¼Œè«‹æŒ‰ã€Œç¹¼çºŒåˆ†æã€ã€‚", reply_markup=REPLY_KB)
        return

    if t == "ç¹¼çºŒåˆ†æ":
        if not room:
            await update.message.reply_text("å°šæœªå»ºç«‹æ¨¡å‹ï¼Œè«‹å…ˆä¸Šå‚³åœ–ç‰‡ã€‚", reply_markup=REPLY_KB); return
        if not room.get("last_input"):
            await update.message.reply_text("è«‹å…ˆæŒ‰ã€ŒèŠ/é–’ã€è¼¸å…¥æœ€æ–°é–‹çã€‚", reply_markup=REPLY_KB); return

        room["history"].append(room["last_input"])
        room["last_input"] = None

        pred = gpt_predict(room["history"])
        await update.message.reply_text(fmt(pred), reply_markup=REPLY_KB)
        return

    await update.message.reply_text("è«‹ç”¨ä¸‹æ–¹å¿«æ·éµæ“ä½œã€‚", reply_markup=REPLY_KB)

# ====== Entry ======
def main():
    app = ApplicationBuilder().token(BOT_TOKEN).build()
    app.add_handler(CommandHandler("start", cmd_start))
    app.add_handler(MessageHandler(filters.PHOTO, on_photo))
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, on_text))

    if WEBHOOK_URL:
        port = int(os.environ.get("PORT", 10000))
        app.run_webhook(listen="0.0.0.0", port=port, url_path=BOT_TOKEN, webhook_url=f"{WEBHOOK_URL}/{BOT_TOKEN}")
    else:
        app.run_polling(allowed_updates=Update.ALL_TYPES)

if __name__ == "__main__":
    main()
